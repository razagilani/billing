#!/usr/bin/env python
'''
Back up ReeBill databases to S3.
'''
from boto.s3.connection import S3Connection
from os.path import basename
import os
from boto.s3.key import Key
from subprocess import call, Popen, PIPE
# from datetime import datetime
from datetime import datetime
import argparse
import os
import shutil
from billing import init_config
init_config()
from billing import config
import re
import sys
import shlex
from StringIO import StringIO
from gzip import GzipFile

# TODO set from command line argument?
BUCKET_NAME = 'skyline-test'

# amount of data to read and send to S3 at one time in bytes
CHUNK_SIZE_BYTES = 5 * 1024**2

#TODO pipe through gzip
# see if check_call can be used
MYSQLDUMP_COMMAND = 'mysqldump -u%(user)s -p%(password)s %(db)s'

MONGODUMP_COMMAND = 'mongodump -d %(db)s -h %(host)s -c %(collection)s -o -'
MONGO_COLLECTIONS = ['users', 'journal']

# extract MySQL connection parameters from connection string in config file
# eg mysql://root:root@localhost:3306/skyline_dev
db_uri = config.get('statedb', 'uri')
m = re.match(r'^mysql://(\w+):(\w+)+@(\w+):([0-9]+)/(\w+)$', db_uri)
db_params = dict(zip(['user', 'password', 'host', 'port', 'db'], m.groups()))


def run_command_with_output_to_s3(command, s3_key):
    '''Run 'command' as a subprocess, writing its stdout to 's3_key'
    (boto.s3.key.Key object). Wait for the process to exit and raise a ValueError
    if it exited with non-0 status.
    
    The upload to S3 is only completed if the process exited with status 0. A
    file should exist on S3 if and only if the command exited with status 0.

    The subprocess' stderr is forwarded to this script's stderr.
    '''
    process = Popen(shlex.split(command), stderr=sys.stderr, stdout=PIPE)

    multipart_upload = bucket.initiate_multipart_upload(s3_key)
    count = 1
    while True:
        chunk = process.stdout.read(CHUNK_SIZE_BYTES)
        if chunk == '':
            break
        multipart_upload.upload_part_from_file(StringIO(chunk), count)
        count += 1 

    status = process.wait()
    if status != 0:
        raise ValueError('Command exited with status %s: "%s"' % (
                status, command))

    multipart_upload.complete_upload()

def backup_mysql(key_name):
    command = MYSQLDUMP_COMMAND % db_params
    key = Key(bucket, name=key_name)
    run_command_with_output_to_s3(command, key)

def backup_mongo_collection(key_name):
    # NOTE "usersdb" section is used to get mongo database parameters for
    # all collections
    command = MONGODUMP_COMMAND % dict(
            db=config.get('usersdb', 'database'),
            host=config.get('usersdb', 'host'),
            collection=collection)
    key = Key(bucket, name=key_name)
    run_command_with_output_to_s3(command, key)

def parse_args():
    parser = argparse.ArgumentParser(
            description="Backup script for utility bill and reebill databases")
    # TODO implement, or use built-in amazon feature that does this
    #parser.add_argument("--limit", default = 5, type=int,
            #help="The max number of backups to store before deleting the oldest")
    # TODO --access-key and --secret-key should be mandatory; how to get argparse to enforce that?
    parser.add_argument("--access-key", type=str,
            help="The S3 access key for authenticating, generated by AWS IAM")
    parser.add_argument("--secret-key", type=str,
            help="The S3 secret key for authenticating, generated by AWS IAM")
    return parser.parse_args()

if __name__ == '__main__':
    now = datetime.utcnow().isoformat()

    args = parse_args()

    #access_key = 'AKIAINC2Y2F7IOLKO6CA'
    #secret_key = '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'
    conn = S3Connection(args.access_key, args.secret_key)
    bucket = conn.get_bucket(BUCKET_NAME)

    backup_mysql('%s_reebill_mysql' % now)
        
    for collection in MONGO_COLLECTIONS:
        backup_mongo_collection('%s_reebill_mongo_%s' % (now, collection))
