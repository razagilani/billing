#!/usr/bin/env python
'''
Back up ReeBill databases to S3.
'''
from boto.s3.connection import S3Connection
from os.path import basename
import os
from boto.s3.key import Key
from subprocess import call, Popen, PIPE
# from datetime import datetime
from datetime import datetime
import argparse
import os
import shutil
from billing import init_config
init_config()
from billing import config
import re
import sys
import shlex

MYSQLDUMP_COMMAND = "mysqldump -u%(user)s -p%(password)s %(db)s"

# TODO "-o -" can be used to write to stdout, but another option needs to be
# used to create only a file instead of a directory for each collection
MONGODUMP_COMMAND = '''
mongodump -d %(db)s -h %(host)s -c %(collection)s -o %(path)s
'''.strip()

MONGO_COLLECTIONS = ['users', 'journal']

db_uri = config.get('statedb', 'uri')
# eg mysql://root:root@localhost:3306/skyline_dev
m = re.match(r'^mysql://(\w+):(\w+)+@(\w+):([0-9]+)/(\w+)$', db_uri)
db_params = dict(zip(['user', 'password', 'host', 'port', 'db'], m.groups()))


def run_command(command, stdout_file=sys.stdout):
    '''Run 'command' as a subprocess, and wait for it to exit. Raise a
    ValueError if it exited with non-0 status.

    If 'stdout_file' (file object) is given, redirect the subprocess' stdout
    If 'stdout_file' (file object) is given, redirect the subprocess' stdout
    to that file instead of forwarding it to this script's stdout.

    The subprocess' stderr is always forwarded to this script's stderr.
    '''
    process = Popen(shlex.split(command), stderr=sys.stderr, stdout=stdout_file)
    status = process.wait()
    if status != 0:
        raise ValueError('Command exited with status %s: "%s"' % (
                status, command))

def dump_mysql(output_file):
    command = MYSQLDUMP_COMMAND % db_params
    run_command(command, stdout_file=output_file)

def dump_mongo_collection_to_file(collection, dump_file_path):
    # NOTE "usersdb" section is used to get mongo collection parameters for
    # all databases.
    command = MONGODUMP_COMMAND % dict(
            db=config.get('usersdb', 'database'),
            host=config.get('usersdb', 'host'),
            collection=collection, path=dump_file_path)
    run_command(command)

if __name__ == '__main__':
     now = datetime.utcnow().isoformat()
     with open('/tmp/backup_test', 'w') as out_file:
        dump_mysql(out_file)

     for collection in MONGO_COLLECTIONS:
         dump_file_path = '/tmp/backup_test_mongo_%s' % collection
         dump_mongo_collection_to_file(collection, dump_file_path)

    #bucket = backup_tools.connect_to_bucket('{0}-backup'.format(
    # args.database), args.access_key, args.secret_key)
    # tmp_dir = "/tmp/"
    # app_name = 'olap'
    #
    # parser = argparse.ArgumentParser(description=
    # "Backup script for {0} database".format(app_name))
    # parser.add_argument("database",
    #     help="The {0} database name to be backed up".format(app_name))
    # parser.add_argument("host",
    #     help="The hostname of the database server being used")
    # parser.add_argument("--limit", default = 5, type=int,
    #     help="The max number of backups to store before deleting the oldest")
    # parser.add_argument("--access_key", default = None, type=str,
    #     help="The S3 access key for authenticating, generated by AWS IAM")
    # parser.add_argument("--secret_key", default = None, type=str,
    #     help="The S3 secret key for authenticating, generated by AWS IAM")
    # args = parser.parse_args()
    # dump_path = tmp_dir + args.database + now
    # backup_tools.dump_mongo_to_file(args.database, args.host, 'facts_alltime', dump_path)
    # backup_tools.dump_mongo_to_file(args.database, args.host, 'facts_daily', dump_path)
    # backup_tools.dump_mongo_to_file(args.database, args.host, 'facts_hourly', dump_path)
    # backup_tools.dump_mongo_to_file(args.database, args.host, 'facts_monthly', dump_path)
    # backup_tools.dump_mongo_to_file(args.database, args.host, 'facts_yearly', dump_path)
    # backup_tools.dump_mongo_to_file(args.database, args.host, 'facts_weekly', dump_path)
    # backup_tools.dump_mongo_to_file(args.database, args.host, 'polysun_projections', dump_path)
    # backup_tools.dump_mongo_to_file(args.database, args.host, 'range_annotations', dump_path)
    # backup_tools.dump_mongo_to_file(args.database, args.host, 'skykit_installs', dump_path)
    # backup_tools.dump_mongo_to_file(args.database, args.host, 'skymap', dump_path)
    #
    # backup_tools.tar_directory(dump_path)
    #
    # bucket = backup_tools.connect_to_bucket('{0}-backup'.format(args.database), args.access_key, args.secret_key)
    #
    # backup_tools.upload_to_bucket(dump_path+".tar.gz", bucket)
    #
    # if backup_tools.size_of_bucket(bucket) > args.limit:
    #     print("bucket contains {0} files, removing oldest".format(backup_tools.size_of_bucket(bucket)))
    #     backup_tools.remove_oldest_backup(bucket)
    #
    # clean up dump files
    #shutil.rmtree(dump_path)
    #os.remove(dump_path+".tar.gz")

