from datetime import datetime
import argparse
import backup_tools
import os
import sys

if __name__ == '__main__':

    now = datetime.now().isoformat()
    tmp_dir = "/tmp/"
    app_name = 'xbill'

    parser = argparse.ArgumentParser(description=
            "Backup script for {0} database".format(app_name))
    parser.add_argument("environment", 
                        help="The {0} database to be backed up".format(app_name)) 
    parser.add_argument("host", 
                        help="The hostname of the database server being used") 
    parser.add_argument("--limit", default = 5, type=int,
                        help="The max number of backups to store before deleting the oldest") 
    parser.add_argument("--access_key", default = None, type=str,
                        help="The S3 access key for authenticating, generated by AWS IAM") 
    parser.add_argument("--secret_key", default = None, type=str,
                        help="The S3 secret key for authenticating, generated by AWS IAM") 

    args = parser.parse_args() 


    dump_path = tmp_dir + args.environment + now

    with file(dump_path, 'w') as dump_file:
        backup_tools.dump_psql_to_file(args.environment,
                                    args.host,
                                    args.environment,
                                    dump_file)

    backup_tools.tar_directory(dump_path)

    bucket = backup_tools.connect_to_bucket('{0}-backup'.format(args.environment), args.access_key, args.secret_key)

    backup_tools.upload_to_bucket(dump_path+".tar", bucket)
    
    if backup_tools.size_of_bucket(bucket) > args.limit:
        print("bucket contains {0} files, removing oldest".format(backup_tools.size_of_bucket(bucket)))
        backup_tools.remove_oldest_backup(bucket)
    
    # clean up dump files
    os.remove(dump_path)
    os.remove(dump_path+".tar")
